# --- Step 1: Import libraries ---
import nltk
from nltk.corpus import treebank
from nltk.tag import UnigramTagger, BigramTagger, DefaultTagger, RegexpTagger
from nltk.tag.hmm import HiddenMarkovModelTagger
import spacy
from sklearn.metrics import classification_report
from tqdm import tqdm
import matplotlib.pyplot as plt


# --- Step 2: Download required datasets ---
nltk.download('treebank')
nltk.download('universal_tagset')

# --- Step 3: Load Treebank dataset ---
sentences = treebank.tagged_sents(tagset='universal')
print(f"âœ… Dataset loaded: {len(sentences)} sentences")

# --- Step 4: Split into train/test ---
train_data = sentences[:3000]
test_data = sentences[3000:]

# ===================================================================
# ğŸ“˜ 1ï¸âƒ£ Rule-Based Tagger (Regular Expressions + Default)
# ===================================================================
patterns = [
    (r'.*ing$', 'VERB'),
    (r'.*ed$', 'VERB'),
    (r'.*es$', 'VERB'),
    (r'.*ould$', 'VERB'),
    (r'.*\'s$', 'NOUN'),
    (r'.*s$', 'NOUN'),
    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),
    (r'.*', 'NOUN')  # default
]
rule_based_tagger = RegexpTagger(patterns)
rule_acc = rule_based_tagger.evaluate(test_data)
print(f"ğŸ”¹ Rule-based Tagger Accuracy: {rule_acc:.3f}")

# ===================================================================
# ğŸ“— 2ï¸âƒ£ Statistical Tagger (Hidden Markov Model)
# ===================================================================
print("Training HMM Tagger... (this may take a minute)")
hmm_tagger = HiddenMarkovModelTagger.train(train_data)
hmm_acc = hmm_tagger.evaluate(test_data)
print(f"ğŸ”¹ HMM Tagger Accuracy: {hmm_acc:.3f}")

# ===================================================================
# ğŸ“™ 3ï¸âƒ£ Deep Learning Tagger (spaCy pretrained)
# ===================================================================
nlp = spacy.load("en_core_web_sm")
def spacy_tagger_accuracy(test_sents):
    gold_tags = []
    pred_tags = []
    for sent in tqdm(test_sents[:500], desc="Evaluating spaCy"):
        tokens = [word for word, tag in sent]
        gold = [tag for word, tag in sent]
        doc = nlp(" ".join(tokens))
        pred = [token.pos_ for token in doc]

        # âœ… ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ø·ÙˆÙ„ Ù…ØªØ³Ø§ÙˆÙŠ (Ø¹Ù„Ø´Ø§Ù† Ù…Ø§ÙŠØ­ØµÙ„Ø´ mismatch)
        if len(pred) == len(gold):
            gold_tags.extend(gold)
            pred_tags.extend(pred)
        else:
            # Skip Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ø§Ø®ØªÙ„Ø§Ù ÙÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²
            continue

    print(f"\nâœ… Evaluated {len(gold_tags)} tokens successfully.")
    return classification_report(gold_tags, pred_tags, zero_division=0)


# ===================================================================
# ğŸ§  Example test sentence
# ===================================================================
sentence = "The little cat sat on the red mat and looked outside."
tokens = sentence.split()

print("\nğŸ§© Example sentence tagging:")
print("\nRule-Based Tagger:")
print(rule_based_tagger.tag(tokens))

print("\nHMM Tagger:")
print(hmm_tagger.tag(tokens))

print("\nspaCy Tagger:")
doc = nlp(sentence)
for token in doc:
    print(f"{token.text:<10} â†’ {token.pos_}")
# Accuracy comparison
methods = ['Rule-based', 'HMM', 'spaCy (Pretrained)']
accuracies = [rule_acc, hmm_acc, 0.97]  # ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ Ø¯Ù‚Ø© spaCy

plt.bar(methods, accuracies, color=['#ff9999','#66b3ff','#99ff99'])
plt.ylim(0,1)
plt.title("POS Tagging Accuracy Comparison")
plt.ylabel("Accuracy")
plt.show()